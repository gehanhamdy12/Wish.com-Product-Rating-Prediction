{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import libraries"
      ],
      "metadata": {
        "id": "avjcwCy6BOOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kXYm80iCVhoU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QZ4mtcZWQRM"
      },
      "source": [
        "#Load train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "slW2x5urWM89",
        "outputId": "568c7f7e-92aa-4f3f-b4e9-494c74147063"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1a9fa5cb-331c-4892-b97e-e307c584ca56\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>retail_price</th>\n",
              "      <th>currency_buyer</th>\n",
              "      <th>units_sold</th>\n",
              "      <th>uses_ad_boosts</th>\n",
              "      <th>rating</th>\n",
              "      <th>rating_count</th>\n",
              "      <th>badges_count</th>\n",
              "      <th>badge_local_product</th>\n",
              "      <th>badge_product_quality</th>\n",
              "      <th>badge_fast_shipping</th>\n",
              "      <th>tags</th>\n",
              "      <th>product_color</th>\n",
              "      <th>product_variation_size_id</th>\n",
              "      <th>product_variation_inventory</th>\n",
              "      <th>shipping_option_name</th>\n",
              "      <th>shipping_option_price</th>\n",
              "      <th>shipping_is_express</th>\n",
              "      <th>countries_shipped_to</th>\n",
              "      <th>inventory_total</th>\n",
              "      <th>has_urgency_banner</th>\n",
              "      <th>urgency_text</th>\n",
              "      <th>origin_country</th>\n",
              "      <th>merchant_title</th>\n",
              "      <th>merchant_name</th>\n",
              "      <th>merchant_info_subtitle</th>\n",
              "      <th>merchant_rating_count</th>\n",
              "      <th>merchant_rating</th>\n",
              "      <th>merchant_id</th>\n",
              "      <th>merchant_has_profile_picture</th>\n",
              "      <th>merchant_profile_picture</th>\n",
              "      <th>theme</th>\n",
              "      <th>crawl_month</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>8.00</td>\n",
              "      <td>7</td>\n",
              "      <td>EUR</td>\n",
              "      <td>10000</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1670</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Summer,soildcolor,Plus Size,Tank,camisole,Tops...</td>\n",
              "      <td>yellow</td>\n",
              "      <td>M</td>\n",
              "      <td>50</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>keepahorse</td>\n",
              "      <td>keepahorse</td>\n",
              "      <td>88 % avis positifs (66,644 notes)</td>\n",
              "      <td>66644</td>\n",
              "      <td>4.137582</td>\n",
              "      <td>577fb2b368116418674befd9</td>\n",
              "      <td>1</td>\n",
              "      <td>https://s3-us-west-1.amazonaws.com/sweeper-pro...</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1515</th>\n",
              "      <td>8.00</td>\n",
              "      <td>7</td>\n",
              "      <td>EUR</td>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>843</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>bathing suit,Plus Size,bikini set,sexy swimsui...</td>\n",
              "      <td>black</td>\n",
              "      <td>L</td>\n",
              "      <td>50</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>shanghaimingqidianqikejiyouxiangongsi</td>\n",
              "      <td>上海铭启电气科技有限公司</td>\n",
              "      <td>91 % avis positifs (25,752 notes)</td>\n",
              "      <td>25752</td>\n",
              "      <td>4.256873</td>\n",
              "      <td>566a3ef17233ff2686443082</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>6.00</td>\n",
              "      <td>34</td>\n",
              "      <td>EUR</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Summer,Vest,momshirt,Get,summer t-shirts,funny...</td>\n",
              "      <td>white</td>\n",
              "      <td>L</td>\n",
              "      <td>50</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>zhaodongmei</td>\n",
              "      <td>zhaodongmei</td>\n",
              "      <td>83 % avis positifs (3,897 notes)</td>\n",
              "      <td>3897</td>\n",
              "      <td>3.928920</td>\n",
              "      <td>5aacd6c1ccf0c83e5a6f3de0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>4.67</td>\n",
              "      <td>4</td>\n",
              "      <td>EUR</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Summer,Shorts,pants,Beach,Plus Size,beachpant,...</td>\n",
              "      <td>lakeblue</td>\n",
              "      <td>XS</td>\n",
              "      <td>5</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>pookie0331</td>\n",
              "      <td>pookie0331</td>\n",
              "      <td>87 % avis positifs (7,497 notes)</td>\n",
              "      <td>7497</td>\n",
              "      <td>4.079365</td>\n",
              "      <td>583141fbfef4094e51453d9b</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>8.00</td>\n",
              "      <td>11</td>\n",
              "      <td>EUR</td>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>127</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Summer,Floral print,women dresses,fashion dres...</td>\n",
              "      <td>apricot</td>\n",
              "      <td>S</td>\n",
              "      <td>50</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Quantité limitée !</td>\n",
              "      <td>CN</td>\n",
              "      <td>shitongyi12683626</td>\n",
              "      <td>shitongyi12683626</td>\n",
              "      <td>91 % avis positifs (173 notes)</td>\n",
              "      <td>173</td>\n",
              "      <td>4.306358</td>\n",
              "      <td>5d3bc9f8e13a7e654424a4cb</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a9fa5cb-331c-4892-b97e-e307c584ca56')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a9fa5cb-331c-4892-b97e-e307c584ca56 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a9fa5cb-331c-4892-b97e-e307c584ca56');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      price  retail_price  ...   theme  crawl_month\n",
              "id                         ...                     \n",
              "272    8.00             7  ...  summer      2020-08\n",
              "1515   8.00             7  ...  summer      2020-08\n",
              "940    6.00            34  ...  summer      2020-08\n",
              "309    4.67             4  ...  summer      2020-08\n",
              "226    8.00            11  ...  summer      2020-08\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Read 'train_new.csv' file\n",
        "train_data=pd.read_csv('train_new.csv',index_col='id')\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toAt_BrRWrHP"
      },
      "source": [
        "#Preprocessing Train Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "our data has some noise and mising values which have bad effect in our model performance .so i will make preprocessing on this data to try to get high performance "
      ],
      "metadata": {
        "id": "ErlxgMhiPx_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiYOSc00XDbI",
        "outputId": "abbf9577-0e3f-40a8-b8fc-8d76d0384e30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "price                             0\n",
              "retail_price                      0\n",
              "currency_buyer                    0\n",
              "units_sold                        0\n",
              "uses_ad_boosts                    0\n",
              "rating                            0\n",
              "rating_count                      0\n",
              "badges_count                      0\n",
              "badge_local_product               0\n",
              "badge_product_quality             0\n",
              "badge_fast_shipping               0\n",
              "tags                              0\n",
              "product_color                    29\n",
              "product_variation_size_id        12\n",
              "product_variation_inventory       0\n",
              "shipping_option_name              0\n",
              "shipping_option_price             0\n",
              "shipping_is_express               0\n",
              "countries_shipped_to              0\n",
              "inventory_total                   0\n",
              "has_urgency_banner              745\n",
              "urgency_text                    745\n",
              "origin_country                   12\n",
              "merchant_title                    0\n",
              "merchant_name                     4\n",
              "merchant_info_subtitle            0\n",
              "merchant_rating_count             0\n",
              "merchant_rating                   0\n",
              "merchant_id                       0\n",
              "merchant_has_profile_picture      0\n",
              "merchant_profile_picture        924\n",
              "theme                             0\n",
              "crawl_month                       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Show number of mising data in each column\n",
        "train_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop columns**\n",
        "\n",
        "'merchant_name' and 'merchant_title' columns  have almost the same data So I droped'merchant_name' column\n",
        "\n",
        "'inventory_total' column has a very little number of data, so I drop it \n",
        "'currency_buyer', 'theme', and 'crawl_month' columns that have the same data all over the rows, so I dropped these columns \n",
        "\n",
        "'badge_fast_shipping','shipping_is_express','urgency_text','merchant_name','merchant_profile_picture' columns that may have no effect on the product's rate,so I dropped them on train data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lxGh3NciNEWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to count each uniqe in column\n",
        "train_data['inventory_total'].value_counts()"
      ],
      "metadata": {
        "id": "mHsQqiajZj_f",
        "outputId": "c0c79abc-725f-4374-b8d1-144f7bd60ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50    1089\n",
              "1        1\n",
              "9        1\n",
              "37       1\n",
              "2        1\n",
              "36       1\n",
              "Name: inventory_total, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t__erOg1XHda"
      },
      "outputs": [],
      "source": [
        "# Drop ('currency_buyer','badge_fast_shipping','shipping_is_express',\n",
        "#       'inventory_total','urgency_text','merchant_name','merchant_profile_picture',\n",
        "#        'merchant_info_subtitle','merchant_id','crawl_month','theme') columns that may have no impacts in the products rating \n",
        "        \n",
        "train_data=train_data.drop(columns=['currency_buyer','badge_fast_shipping','shipping_is_express',\n",
        "                                    'inventory_total','urgency_text','merchant_name','merchant_profile_picture',\n",
        "                                    'merchant_info_subtitle','merchant_id','crawl_month','theme'])                   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop rows**\n",
        "\n",
        "Becouse of  the Ratings are in categories from 1 to 5 and I found only one row that has rating=6, so i will drop this row \n",
        "in general I don't prefer to drop rows because it may affect our data but here it is only one row\n"
      ],
      "metadata": {
        "id": "eYPzXVA-Tkci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To show the unique value in (rating) column and how many each uniqe is been reppeated\n",
        "train_data.loc[:, 'rating'].unique()\n",
        "train_data.loc[:, 'rating'].value_counts()"
      ],
      "metadata": {
        "id": "1EvKVKZvUH5U",
        "outputId": "3c8ae607-6c03-43a8-a5f0-79922149b2db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0    774\n",
              "5.0    170\n",
              "3.0    135\n",
              "2.0     12\n",
              "1.0      2\n",
              "6.0      1\n",
              "Name: rating, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop row that has rating=6\n",
        "train_data = train_data.drop(train_data[train_data.rating == 6].index)"
      ],
      "metadata": {
        "id": "xmxB-0SvTr8r"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data imputation**\n",
        "\n",
        "there are columns that have mising values So I fill columns that have categorical data with (other) like ('product_color','product_variation_size_id','origin_country) columns\n",
        "\n",
        "has_urgency_banner' column must be filled with 0.because  there is text in the 'urgency text' column, each row of this column equals one. As a result so the empty rows in the 'has urgency banner' column must be filled with 0. \n",
        "on train data\n"
      ],
      "metadata": {
        "id": "w5_Q4tfUWVOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lJ40ZuqTXNPU"
      },
      "outputs": [],
      "source": [
        "# Fill mising value in (has_urgency_banner) column with zero\n",
        "# Fill mising value in (product_color) column with other\n",
        "# Fill mising value in (product_variation_size_id)column with other\n",
        "# Fill mising value in (origin_country)column with other\n",
        "\n",
        "train_data['has_urgency_banner'] = train_data['has_urgency_banner'].fillna(0)\n",
        "train_data['product_variation_size_id'] = train_data['product_variation_size_id'].fillna('other')\n",
        "train_data['product_color'] = train_data['product_color'].fillna('other')\n",
        "train_data['origin_country'] = train_data['origin_country'].fillna('other')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkt7SoQVXTMh",
        "outputId": "bb899e51-853a-4460-d2df-c68e16a39fd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "black               219\n",
              "white               173\n",
              "blue                 77\n",
              "yellow               69\n",
              "pink                 68\n",
              "                   ... \n",
              "claret                1\n",
              "brown & yellow        1\n",
              "violet                1\n",
              "Pink                  1\n",
              "navyblue & white      1\n",
              "Name: product_color, Length: 87, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# To find how many uniqe variable in (product_color) column and count how many each uniqe\n",
        "train_data.loc[:, 'product_color'].unique()\n",
        "train_data.loc[:, 'product_color'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**preprocessing (product_color)** \n",
        "\n",
        "Handling variable in (product_color) column by collecting all the same colors but have different names to replace them by the common name\n",
        "(ex.)'watermelonred','rosered','lightred' replace to red in train data"
      ],
      "metadata": {
        "id": "wKol-CWwcuew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-43W1BHrXeHv"
      },
      "outputs": [],
      "source": [
        "# preprocessing (product_color) colmn (handling its values) \n",
        "\n",
        "train_data.replace(to_replace= ['white','White','whitefloral','offwhite','ivory'], value = 'white', inplace=True)\n",
        "\n",
        "train_data.replace(to_replace= ['blue','Blue','lakeblue','lightblue','skyblue','darkblue','denimblue'], value = 'blue')\n",
        "train_data.replace(to_replace= ['navy','navy blue'], value = 'navyblue', inplace=True)\n",
        "\n",
        "train_data.replace(to_replace= ['darkgreen','fluorescentgreen','mintgreen','applegreen','lightgreen','light green'], value = 'green')\n",
        "train_data.replace(to_replace= ['Army green'], value = 'armygreen', inplace=True)\n",
        "\n",
        "train_data.replace(to_replace= ['red','RED','watermelonred','rosered','lightred','burgundy','claret'], value = 'red')\n",
        "train_data.replace(to_replace= ['wine red','wine'], value = 'winered', inplace=True)\n",
        "\n",
        "train_data.replace(to_replace= ['black','Black','coolblack'], value = 'black', inplace=True )\n",
        "\n",
        "train_data.replace(to_replace= ['pink','Pink','lightpink','rose','dustypink','rosegold','dustypink'], value = 'pink',inplace=True)\n",
        "\n",
        "train_data.replace(to_replace= ['army','leopard','white & green','pink & black','pink & white','white & green','black & yellow',\n",
        "                                'star','camouflage','whitestripe','navyblue & white',\n",
        "                                'black & blue','pink & blue','rainbow','floral','brown & yellow',\n",
        "                                'blue & pink','orange-red','winered & yellow','blackwhite'], value = 'other_colores',inplace=True)\n",
        "\n",
        "train_data.replace(to_replace= ['grey','lightgrey'], value = 'gray', inplace=True)\n",
        "\n",
        "train_data.replace(to_replace= ['lightyellow','gold','camel'], value = 'yellow',inplace=True)\n",
        "\n",
        "column_values = train_data[[\"product_color\"]].values.ravel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**preprocessing (product_variation_size_id)**\n",
        "\n",
        "Handling variable in (product_variation_size_id) column by collecting all the same size but have different names to replace them by the common name\n",
        "\n",
        " (ex.) ['XS', 'XS.','Size-XS', 'SIZE XS']  repleace to 'XS' and so on and the variables which do not refer to size collect them to variable called (Others) in train data"
      ],
      "metadata": {
        "id": "863p2GS9byG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrGj14LgX39n",
        "outputId": "a2fe9122-8807-4cf9-e38f-75210c983447"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['M', 'L', 'XS', 'S', 'XL', 'Others', '2XS', 'other', '3XS', '2XL',\n",
              "       '4XL', '5XL'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# preprocessing (product_variation_size_id) colmn (handling its values)\n",
        "\n",
        "train_data.replace(to_replace= ['S.','S(bust 88cm)', 's', 'Size S', 'Size-S', 'S/M(child)', 'S (waist58-62cm)', 'S Pink','US-S', \n",
        "         'pants-S','Size S.', 'S..','25-S', 'size S', 'Size/S', 'Suit-S', 'SIZE S'], value = 'S', inplace=True )\n",
        "train_data.replace(to_replace= ['XXS','Size-XXS', 'SIZE XXS','Size -XXS', 'Size XXS','SIZE-XXS'], value = '2XS', inplace=True )\n",
        "train_data.replace(to_replace= ['M','Size M','M.'], value = 'M', inplace=True )\n",
        "train_data.replace(to_replace= ['L','SizeL'], value = 'L', inplace=True )\n",
        "train_data.replace(to_replace= ['XS', 'XS.','Size-XS', 'SIZE XS'], value = 'XS', inplace=True )\n",
        "train_data.replace(to_replace= ['XL', 'X   L', '1 PC - XL'], value = 'XL', inplace=True )\n",
        "train_data.replace(to_replace= ['XXL', '2XL'], value = '2XL', inplace=True )\n",
        "train_data.replace(to_replace= ['4XL','XXXXL','Size4XL'], value = '4XL', inplace=True )\n",
        "train_data.replace(to_replace= ['XXXXXL', 'Size-5XL','5XL'], value = '5XL', inplace=True )\n",
        "train_data.replace(to_replace= ['XXXS'], value = '3XS', inplace=True )\n",
        "\n",
        "train_data.replace(to_replace= ['H01','1pc','34','Floating Chair for Kid','60','100 cm','6XL','5PAIRS',\n",
        "                               '1','100 x 100cm(39.3 x 39.3inch)','Base Coat','choose a size','26(Waist 72cm 28inch)',\n",
        "                               'Pack of 1','Round','5','10pcs','40 cm','white','1m by 3m ',\n",
        "                               '4-5 Years','Base & Top & Matte Top Coat','20pcs','B','Baby Float Boat',\n",
        "                               'One Size','30 cm','20PCS-10PAIRS','2','2pcs','25','29','33',\n",
        "                               '35','10 ml','Women Size 36','1m by 3m','1 pc.','3XL','04-3XL'], value = 'Others', inplace=True )\n",
        "\n",
        "size_column = train_data[[\"product_variation_size_id\"]].values.ravel()\n",
        "unique_value =  pd.unique(size_column)\n",
        "unique_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcMtH-4PYEys",
        "outputId": "acb7737e-554e-4a10-904b-cb262b5395f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Livraison standard         1047\n",
              "Standard Shipping            16\n",
              "Envio Padrão                  6\n",
              "الشحن القياسي                 4\n",
              "Expediere Standard            4\n",
              "Envío normal                  3\n",
              "Standardversand               3\n",
              "Standardowa wysyłka           2\n",
              "Standart Gönderi              2\n",
              "Livraison Express             2\n",
              "Стандартная доставка          2\n",
              "Spedizione standard           1\n",
              "ការដឹកជញ្ជូនតាមស្តង់ដារ       1\n",
              "Name: shipping_option_name, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# To find how many uniqe variable in (shipping_option_name) column and count how many each uniqe\n",
        "train_data.loc[:, 'shipping_option_name'].unique()\n",
        "train_data.loc[:, 'shipping_option_name'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**shipping_is_express column**\n",
        "\n",
        "Handling variable in (shipping_is_express) column by replacing the variable ['Standard Shipping','الشحن القياسي','Expediere Standard','Standardversand','Standardowa wysyłka',\n",
        "'Standart Gönderi','Стандартная доставка','Spedizione standard','ការដឹកជញ្ជូនតាមស្តង់ដារ'] that have the same meaning with 'Livraison standard' in training  "
      ],
      "metadata": {
        "id": "U_oiOqq9bKsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "36h0V3WdYJ7F"
      },
      "outputs": [],
      "source": [
        "# preprocessing (shipping_option_name) colmn (handling its values)\n",
        "#Handling variable in (shipping_is_express) column by replacing the variable ['Standard Shipping','الشحن القياسي','Expediere Standard','Standardversand','Standardowa wysyłka',\n",
        "#'Standart Gönderi','Стандартная доставка','Spedizione standard','ការដឹកជញ្ជូនតាមស្តង់ដារ'] that have the same meaning with 'Livraison standard' in training & testing data \n",
        "\n",
        "train_data.replace(to_replace= ['Standard Shipping','الشحن القياسي','Expediere Standard',\n",
        "                               'Standardversand','Standardowa wysyłka','Standart Gönderi',\n",
        "                               'Стандартная доставка','Spedizione standard',\n",
        "                                'ការដឹកជញ្ជូនតាមស្តង់ដារ'], value = 'Livraison standard', inplace=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHEkquYhYUUg",
        "outputId": "721a11d6-3b93-4335-de2e-78baeec7abca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "price                           0\n",
              "retail_price                    0\n",
              "units_sold                      0\n",
              "uses_ad_boosts                  0\n",
              "rating                          0\n",
              "rating_count                    0\n",
              "badges_count                    0\n",
              "badge_local_product             0\n",
              "badge_product_quality           0\n",
              "tags                            0\n",
              "product_color                   0\n",
              "product_variation_size_id       0\n",
              "product_variation_inventory     0\n",
              "shipping_option_name            0\n",
              "shipping_option_price           0\n",
              "countries_shipped_to            0\n",
              "has_urgency_banner              0\n",
              "origin_country                  0\n",
              "merchant_title                  0\n",
              "merchant_rating_count           0\n",
              "merchant_rating                 0\n",
              "merchant_has_profile_picture    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# To ensure that there is no mising data in columns \n",
        "train_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5GTD61fYX9S",
        "outputId": "a4c5bf2b-52dd-46de-eb44-e3ffdbbd0a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1093 entries, 272 to 1536\n",
            "Data columns (total 22 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   price                         1093 non-null   float64\n",
            " 1   retail_price                  1093 non-null   int64  \n",
            " 2   units_sold                    1093 non-null   int64  \n",
            " 3   uses_ad_boosts                1093 non-null   int64  \n",
            " 4   rating                        1093 non-null   float64\n",
            " 5   rating_count                  1093 non-null   int64  \n",
            " 6   badges_count                  1093 non-null   int64  \n",
            " 7   badge_local_product           1093 non-null   int64  \n",
            " 8   badge_product_quality         1093 non-null   int64  \n",
            " 9   tags                          1093 non-null   object \n",
            " 10  product_color                 1093 non-null   object \n",
            " 11  product_variation_size_id     1093 non-null   object \n",
            " 12  product_variation_inventory   1093 non-null   int64  \n",
            " 13  shipping_option_name          1093 non-null   object \n",
            " 14  shipping_option_price         1093 non-null   int64  \n",
            " 15  countries_shipped_to          1093 non-null   int64  \n",
            " 16  has_urgency_banner            1093 non-null   float64\n",
            " 17  origin_country                1093 non-null   object \n",
            " 18  merchant_title                1093 non-null   object \n",
            " 19  merchant_rating_count         1093 non-null   int64  \n",
            " 20  merchant_rating               1093 non-null   float64\n",
            " 21  merchant_has_profile_picture  1093 non-null   int64  \n",
            "dtypes: float64(4), int64(12), object(6)\n",
            "memory usage: 196.4+ KB\n"
          ]
        }
      ],
      "source": [
        "# find out all data Type\n",
        "# To prints information about the train data\n",
        "train_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**categorical data**\n",
        "\n",
        "All input and output variables in machine learning models must be numeric, which means that I must convert categorical data to numbers before fitting and evaluating a model wherefore I used label encoder to convert all categorical data to numerical"
      ],
      "metadata": {
        "id": "WINa9LU8ZDOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ImBsKzAPYche"
      },
      "outputs": [],
      "source": [
        "# To convert all categorical data into numerical data (Train data)\n",
        "el = LabelEncoder()\n",
        "x_coulmuns = ['tags','product_color','product_variation_size_id','shipping_option_name','origin_country','merchant_title']\n",
        "for i in x_coulmuns:\n",
        "   train_data.loc[:, i] = el.fit_transform(train_data[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VomX0IvmY3Ss"
      },
      "source": [
        "#Load test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "g2sFb_lVY4gO",
        "outputId": "bc7af603-c354-4f57-abdb-981057af824e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6ea74d8a-64fc-42c0-a5ea-1fad948e66af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>retail_price</th>\n",
              "      <th>currency_buyer</th>\n",
              "      <th>units_sold</th>\n",
              "      <th>uses_ad_boosts</th>\n",
              "      <th>rating_count</th>\n",
              "      <th>badges_count</th>\n",
              "      <th>badge_local_product</th>\n",
              "      <th>badge_product_quality</th>\n",
              "      <th>badge_fast_shipping</th>\n",
              "      <th>tags</th>\n",
              "      <th>product_color</th>\n",
              "      <th>product_variation_size_id</th>\n",
              "      <th>product_variation_inventory</th>\n",
              "      <th>shipping_option_name</th>\n",
              "      <th>shipping_option_price</th>\n",
              "      <th>shipping_is_express</th>\n",
              "      <th>countries_shipped_to</th>\n",
              "      <th>inventory_total</th>\n",
              "      <th>has_urgency_banner</th>\n",
              "      <th>urgency_text</th>\n",
              "      <th>origin_country</th>\n",
              "      <th>merchant_title</th>\n",
              "      <th>merchant_name</th>\n",
              "      <th>merchant_info_subtitle</th>\n",
              "      <th>merchant_rating_count</th>\n",
              "      <th>merchant_rating</th>\n",
              "      <th>merchant_id</th>\n",
              "      <th>merchant_has_profile_picture</th>\n",
              "      <th>merchant_profile_picture</th>\n",
              "      <th>theme</th>\n",
              "      <th>crawl_month</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>5.75</td>\n",
              "      <td>5</td>\n",
              "      <td>EUR</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Funny,letter print,Fashion,Shirt,Sleeve,Summer...</td>\n",
              "      <td>white</td>\n",
              "      <td>XS</td>\n",
              "      <td>5</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>139</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>liyang163</td>\n",
              "      <td>liyang163</td>\n",
              "      <td>33 % avis positifs (3 notes)</td>\n",
              "      <td>3</td>\n",
              "      <td>2.333333</td>\n",
              "      <td>5e63469b2fdc774466e15dd5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>7.00</td>\n",
              "      <td>7</td>\n",
              "      <td>EUR</td>\n",
              "      <td>5000</td>\n",
              "      <td>0</td>\n",
              "      <td>579</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Women Vest,Vest,Fashion,Women Blouse,long dres...</td>\n",
              "      <td>black</td>\n",
              "      <td>XS</td>\n",
              "      <td>50</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>singing  the  song</td>\n",
              "      <td>singingthesong</td>\n",
              "      <td>86 % avis positifs (4,213 notes)</td>\n",
              "      <td>4213</td>\n",
              "      <td>4.058391</td>\n",
              "      <td>583547f417dc6224cb464117</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>5.00</td>\n",
              "      <td>16</td>\n",
              "      <td>EUR</td>\n",
              "      <td>1000</td>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hollowouttanktop,Tanktops for women,Plus Size,...</td>\n",
              "      <td>blue</td>\n",
              "      <td>XS</td>\n",
              "      <td>20</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>Surper boutique</td>\n",
              "      <td>surperboutique</td>\n",
              "      <td>85 % avis positifs (10,501 notes)</td>\n",
              "      <td>10501</td>\n",
              "      <td>4.029235</td>\n",
              "      <td>57086fd7fa6bee434d0e5852</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>913</th>\n",
              "      <td>18.00</td>\n",
              "      <td>27</td>\n",
              "      <td>EUR</td>\n",
              "      <td>5000</td>\n",
              "      <td>1</td>\n",
              "      <td>1903</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>men jeans,Shorts,Summer,Vintage,Denim,Men,casu...</td>\n",
              "      <td>blue</td>\n",
              "      <td>M</td>\n",
              "      <td>50</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>postonny</td>\n",
              "      <td>postonny</td>\n",
              "      <td>86 % avis positifs (15,855 notes)</td>\n",
              "      <td>15855</td>\n",
              "      <td>4.052917</td>\n",
              "      <td>5a17b55d1f4d8c4f0b44b0ce</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>12.00</td>\n",
              "      <td>81</td>\n",
              "      <td>EUR</td>\n",
              "      <td>20000</td>\n",
              "      <td>0</td>\n",
              "      <td>1799</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Shorts,Lace,pants,Dress,Women Leggings,Hot pan...</td>\n",
              "      <td>white</td>\n",
              "      <td>M</td>\n",
              "      <td>50</td>\n",
              "      <td>Livraison standard</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CN</td>\n",
              "      <td>fashionforgirls</td>\n",
              "      <td>guangzhouchanny</td>\n",
              "      <td>88 % avis positifs (151,914 notes)</td>\n",
              "      <td>151914</td>\n",
              "      <td>4.127921</td>\n",
              "      <td>53aa664438d3046ee44a5024</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>summer</td>\n",
              "      <td>2020-08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ea74d8a-64fc-42c0-a5ea-1fad948e66af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ea74d8a-64fc-42c0-a5ea-1fad948e66af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ea74d8a-64fc-42c0-a5ea-1fad948e66af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     price  retail_price  ...   theme  crawl_month\n",
              "id                        ...                     \n",
              "937   5.75             5  ...  summer      2020-08\n",
              "270   7.00             7  ...  summer      2020-08\n",
              "308   5.00            16  ...  summer      2020-08\n",
              "913  18.00            27  ...  summer      2020-08\n",
              "403  12.00            81  ...  summer      2020-08\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# read 'test_new.csv' file\n",
        "test_data=pd.read_csv('test_new.csv',index_col='id')\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCbMn5sdZcHX"
      },
      "source": [
        "#Preprocessing Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As on train data our test data  has some noise and mising values which have bad effect in our model performance .so i will make preprocessing on this data to try to get high performance"
      ],
      "metadata": {
        "id": "wljI52G3eZwl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3O4eGo1ZPKs",
        "outputId": "cfdaf057-20a4-440e-d1d0-3a50d6fe4854"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "price                             0\n",
              "retail_price                      0\n",
              "currency_buyer                    0\n",
              "units_sold                        0\n",
              "uses_ad_boosts                    0\n",
              "rating_count                      0\n",
              "badges_count                      0\n",
              "badge_local_product               0\n",
              "badge_product_quality             0\n",
              "badge_fast_shipping               0\n",
              "tags                              0\n",
              "product_color                    12\n",
              "product_variation_size_id         2\n",
              "product_variation_inventory       0\n",
              "shipping_option_name              0\n",
              "shipping_option_price             0\n",
              "shipping_is_express               0\n",
              "countries_shipped_to              0\n",
              "inventory_total                   0\n",
              "has_urgency_banner              355\n",
              "urgency_text                    355\n",
              "origin_country                    5\n",
              "merchant_title                    0\n",
              "merchant_name                     0\n",
              "merchant_info_subtitle            1\n",
              "merchant_rating_count             0\n",
              "merchant_rating                   0\n",
              "merchant_id                       0\n",
              "merchant_has_profile_picture      0\n",
              "merchant_profile_picture        423\n",
              "theme                             0\n",
              "crawl_month                       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#to know which colimns that have mising values\n",
        "test_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop columns**\n",
        "\n",
        "'merchant_name' and 'merchant_title' columns  have almost the same data So I droped'merchant_name' column\n",
        "\n",
        "'inventory_total' column has a very little number of data, so I drop it \n",
        "'currency_buyer', 'theme', and 'crawl_month' columns that have the same data all over the rows, so I dropped these columns \n",
        "\n",
        "'badge_fast_shipping','shipping_is_express','urgency_text','merchant_name','merchant_profile_picture' columns that may have no effect on the product's rate,so I dropped them on test data"
      ],
      "metadata": {
        "id": "Lc4pxBhFeELt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9_xJSJt8ZsT9"
      },
      "outputs": [],
      "source": [
        "# Drop (currency_buyer','badge_fast_shipping','shipping_is_express',\n",
        "#                                    'inventory_total','urgency_text','merchant_name','merchant_profile_picture',\n",
        "#                                    'merchant_info_subtitle','merchant_id','crawl_month','theme)column\n",
        "\n",
        "test_data=test_data.drop(columns=['currency_buyer','badge_fast_shipping','shipping_is_express',\n",
        "                                    'inventory_total','urgency_text','merchant_name','merchant_profile_picture',\n",
        "                                    'merchant_info_subtitle','merchant_id','crawl_month','theme'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data imputation**\n",
        "\n",
        "there are columns that have mising values So I fill columns that have categorical data with (other) like ('product_color','product_variation_size_id','origin_country) columns\n",
        "\n",
        "has_urgency_banner' column must be filled with 0.because  there is text in the 'urgency text' column, each row of this column equals one. As a result so the empty rows in the 'has urgency banner' column must be filled with 0. \n",
        "on test data\n"
      ],
      "metadata": {
        "id": "MRLS2uM9dn4O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Jg2kyvciZxcs"
      },
      "outputs": [],
      "source": [
        "# Data imputation\n",
        "# Fill mising value in (has_urgency_banner) column with zero\n",
        "# Fill mising value in (product_color) column with ffill\n",
        "# Fill mising value in (product_variation_size_id)column with other\n",
        "# Fill mising value in (origin_country)column with other\n",
        "\n",
        "test_data['has_urgency_banner'] = test_data['has_urgency_banner'].fillna(0)\n",
        "test_data['product_color'] = test_data['product_color'].fillna('other')\n",
        "test_data['product_variation_size_id'] = test_data['product_variation_size_id'].fillna('other')\n",
        "test_data['origin_country'] = test_data['origin_country'].fillna('other_country')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**preprocessing (product_color) column**\n",
        "\n",
        "Handling variable in (product_color) column by collecting all the same colors but have different names to replace them by the common name\n",
        "(ex.)'watermelonred','rosered','lightred' replace to red in test data"
      ],
      "metadata": {
        "id": "pknv19AfdBe2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IHX4RjK3aHPl"
      },
      "outputs": [],
      "source": [
        "# preprocessing (product_color) colmn (handling its values)\n",
        "# Handling variable in (product_color) column by collecting all the same colors but have different names to replace them by the common name\n",
        "#(ex.)'watermelonred','rosered','lightred' replace to red\n",
        "test_data.replace(to_replace= ['white','White','whitefloral','offwhite'], value = 'white', inplace=True)\n",
        "\n",
        "test_data.replace(to_replace= ['blue','Blue','lakeblue','lightblue','skyblue','darkblue','denimblue'], value = 'blue')\n",
        "test_data.replace(to_replace= ['navy','navy blue'], value = 'navyblue', inplace=True)\n",
        "\n",
        "test_data.replace(to_replace= ['darkgreen','fluorescentgreen','mintgreen','applegreen','lightgreen','light green'], value = 'green')\n",
        "test_data.replace(to_replace= ['Army green'], value = 'armygreen', inplace=True)\n",
        "\n",
        "test_data.replace(to_replace= ['red','RED','watermelonred','rosered','lightred','burgundy','claret','wine'], value = 'red')\n",
        "test_data.replace(to_replace= ['wine red'], value = 'winered', inplace=True)\n",
        "\n",
        "test_data.replace(to_replace= ['black','Black','coolblack'], value = 'black', inplace=True )\n",
        "\n",
        "test_data.replace(to_replace= ['pink','Pink','lightpink','rose','dustypink','rosegold','dustypink'], value = 'pink',inplace=True)\n",
        "\n",
        "test_data.replace(to_replace= ['army','leopard','white & green','pink & black','pink & white','white & green','black & yellow',\n",
        "                                'star','camouflage','whitestripe','navyblue & white',\n",
        "                                'black & blue','pink & blue','rainbow','floral','brown & yellow',\n",
        "                                'blue & pink','orange-red','winered & yellow','blackwhite'], value = 'other_colores',inplace=True)\n",
        "\n",
        "test_data.replace(to_replace= ['grey','lightgrey'], value = 'gray', inplace=True)\n",
        "\n",
        "test_data.replace(to_replace= ['lightyellow','gold','camel'], value = 'yellow',inplace=True)\n",
        "\n",
        "test_data.replace(to_replace= ['nude','ivory'], value = 'beige',inplace=True)\n",
        "\n",
        "\n",
        "column_values = test_data[[\"product_color\"]].values.ravel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**preprocessing (product_variation_size_id)**\n",
        "\n",
        "Handling variable in (product_variation_size_id) column by collecting all the same size but have different names to replace them by the common name\n",
        "\n",
        "(ex.) ['XS', 'XS.','Size-XS', 'SIZE XS']  repleace to 'XS' and so on and the variables which do not refer to size collect them to variable called (Others) in test data"
      ],
      "metadata": {
        "id": "S1uaOhflcdc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kbcoatAyaSVP"
      },
      "outputs": [],
      "source": [
        "# preprocessing (product_variation_size_id) colmn (handling its values)\n",
        "#Handling variable in (product_variation_size_id) column by collecting all the same size but have different names to replace them by the common name\n",
        "#(ex.) ['XS', 'XS.','Size-XS', 'SIZE XS']  repleace to 'XS' and so on and the variables which do not refer to size collect them to variable called (Others) in train data\n",
        "\n",
        "test_data.replace(to_replace= ['S.','S(bust 88cm)', 's', 'Size S', 'Size-S', 'S/M(child)', 'S (waist58-62cm)', 'S Pink','US-S', \n",
        "         'pants-S','Size S.', 'S..','25-S', 'size S', 'Size/S', 'Suit-S', 'SIZE S'], value = 'S', inplace=True )\n",
        "test_data.replace(to_replace= ['XXS','Size-XXS', 'SIZE XXS','Size -XXS', 'Size XXS','SIZE-XXS'], value = '2XS', inplace=True )\n",
        "test_data.replace(to_replace= ['M','Size M','M.'], value = 'M', inplace=True )\n",
        "test_data.replace(to_replace= ['L','SizeL'], value = 'L', inplace=True )\n",
        "test_data.replace(to_replace= ['XS', 'XS.','Size-XS', 'SIZE XS'], value = 'XS', inplace=True )\n",
        "test_data.replace(to_replace= ['XL', 'X   L', '1 PC - XL'], value = 'XL', inplace=True )\n",
        "test_data.replace(to_replace= ['XXL', '2XL'], value = '2XL', inplace=True )\n",
        "test_data.replace(to_replace= ['4XL','XXXXL','Size4XL'], value = '4XL', inplace=True )\n",
        "test_data.replace(to_replace= ['XXXXXL', 'Size-5XL','5XL'], value = '5XL', inplace=True )\n",
        "test_data.replace(to_replace= ['XXXS'], value = '3XS', inplace=True )\n",
        "test_data.replace(to_replace= ['XXXS'], value = '3XS', inplace=True )\n",
        "\n",
        "test_data.replace(to_replace= ['H01','1pc','34','Floating Chair for Kid','60','100 cm','6XL','5PAIRS',\n",
        "                               '1','100 x 100cm(39.3 x 39.3inch)','Base Coat','choose a size','26(Waist 72cm 28inch)',\n",
        "                               'Pack of 1','Round','5','10pcs','40 cm','white','1m by 3m ',\n",
        "                               '4-5 Years','Base & Top & Matte Top Coat','20pcs','B','Baby Float Boat',\n",
        "                               'One Size','30 cm','20PCS-10PAIRS','2','2pcs','25','29','33',\n",
        "                               '35','10 ml','Women Size 36','1m by 3m','1 pc.','3XL','04-3XL'], value = 'Others', inplace=True )\n",
        "\n",
        "column_values = test_data[[\"product_variation_size_id\"]].values.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**preprocessing shipping_is_express column**\n",
        "\n",
        "Handling variable in (shipping_is_express) column by replacing the variable ['Standard Shipping','الشحن القياسي','Expediere Standard','Standardversand','Standardowa wysyłka',\n",
        "'Standart Gönderi','Стандартная доставка','Spedizione standard','ការដឹកជញ្ជូនតាមស្តង់ដារ'] that have the same meaning with 'Livraison standard' in test data "
      ],
      "metadata": {
        "id": "ox-bu4TQblGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1HRyHjiOaWVY"
      },
      "outputs": [],
      "source": [
        "# preprocessing (shipping_is_express) colmn (handling its values)\n",
        "#Handling variable in (shipping_is_express) column by replacing the variable ['Standard Shipping','الشحن القياسي','Expediere Standard','Standardversand','Standardowa wysyłka',\n",
        "#'Standart Gönderi','Стандартная доставка','Spedizione standard','ការដឹកជញ្ជូនតាមស្តង់ដារ'] that have the same meaning with 'Livraison standard' \n",
        "\n",
        "test_data.replace(to_replace= ['Standard Shipping','الشحن القياسي','Expediere Standard',\n",
        "                               'Standardversand','Standardowa wysyłka','Standart Gönderi',\n",
        "                               'Стандартная доставка','Spedizione standard',\n",
        "                                'ការដឹកជញ្ជូនតាមស្តង់ដារ'], value = 'Livraison standard', inplace=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3G8f7xbaatB",
        "outputId": "cb2a4cfd-db47-45c7-9d1f-77281eff61d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "price                           0\n",
              "retail_price                    0\n",
              "units_sold                      0\n",
              "uses_ad_boosts                  0\n",
              "rating_count                    0\n",
              "badges_count                    0\n",
              "badge_local_product             0\n",
              "badge_product_quality           0\n",
              "tags                            0\n",
              "product_color                   0\n",
              "product_variation_size_id       0\n",
              "product_variation_inventory     0\n",
              "shipping_option_name            0\n",
              "shipping_option_price           0\n",
              "countries_shipped_to            0\n",
              "has_urgency_banner              0\n",
              "origin_country                  0\n",
              "merchant_title                  0\n",
              "merchant_rating_count           0\n",
              "merchant_rating                 0\n",
              "merchant_has_profile_picture    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# To ensure that there is no mising data in columns in Testing_data\n",
        "test_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "categorical data\n",
        "\n",
        "All input and output variables in machine learning models must be numeric, which means that I must convert categorical data to numbers before fitting and evaluating a model wherefore I used label encoder to convert all categorical data to numerical"
      ],
      "metadata": {
        "id": "18LXbk_Wa9gF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8NcpJYQ3aeWh"
      },
      "outputs": [],
      "source": [
        "# To convert all categorical data into numerical data (Test data)\n",
        "\n",
        "el = LabelEncoder()\n",
        "x_coulmuns = ['tags','product_color','product_variation_size_id','shipping_option_name','origin_country','merchant_title']\n",
        "for i in x_coulmuns:\n",
        "    test_data.loc[:, i] = el.fit_transform(test_data[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0zM2jEYbIjC"
      },
      "source": [
        "#Split Data "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rating column**\n",
        "\n",
        "I moved (rating) column to last position to make it easier when split data into train and test"
      ],
      "metadata": {
        "id": "Ako_3GtKevUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#move rating column to the last column to make data spliting easiar\n",
        "\n",
        "reorder_column = train_data.pop('rating')\n",
        "train_data.insert(len(train_data. columns), 'rating', reorder_column)"
      ],
      "metadata": {
        "id": "_ZGuzVkMU_k5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_uWW7a0CbP6C"
      },
      "outputs": [],
      "source": [
        "# set all features in train data except (rating column) in (X) variable\n",
        "# set rating column in variable(y)\n",
        "X=train_data.iloc[:,:-1]\n",
        "y=train_data.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The experimental protocol**\n",
        "\n",
        "The experimental protocol I used is hold out method (train_test_split) function \n",
        "\n",
        "using train_test_split to split train data into (X_train, X_test, y_train, y_test) to fit the models and make a prediction\n",
        "\n",
        "train=0.85 , test=0.15"
      ],
      "metadata": {
        "id": "I3kM0QO8faAX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TMUvlzv_biyA"
      },
      "outputs": [],
      "source": [
        "# split train data into (X_train, X_test, y_train, y_test) to fit the models and make a prediction\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.15, random_state=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39lh2CLaatU5"
      },
      "source": [
        "# Decision Tree  Model 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvEFgiEnkmMp"
      },
      "source": [
        "**Because of its resistance to noise, tolerance for missing information, management of irrelevant,redundant predictive attribute values,and low computational cost,So decision Tree is one of the most extensively used Machine Learning Algorithms.**\n",
        "\n",
        "My question is how to tune a Decision Tree. What should be the range of values I should try for the maximum depth, which criteria shoulld I use (\"gini\",\"entropy\")? which  strategy  should use to choose the split at each node.(“best” or “random” )?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6QqngkVa2kI",
        "outputId": "320c2835-af76-4af7-b438-38a736a79215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction on training data : 0.9989235737351991\n",
            "prediction on test part from training data : 0.6707317073170732\n"
          ]
        }
      ],
      "source": [
        "# Decision Tree Classifier without using hyperparametar\n",
        "\n",
        "#import DecisionTreeClassifier from sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# define decission tree model\n",
        "decission_tree=DecisionTreeClassifier()\n",
        "# fit model with train data\n",
        "decission_tree.fit(X_train, y_train)\n",
        "# make prediction on training data to know how much the model learn from train data\n",
        "y_hat2=decission_tree.predict(X_train)\n",
        "# predict test part from (train data)\n",
        "y_hat=decission_tree.predict(X_test)\n",
        "\n",
        "# to check if there is overfitting or no)\n",
        "print(\"prediction on training data :\",f1_score(y_train, y_hat2, average='micro'))\n",
        "print(\"prediction on test part from training data :\",f1_score(y_test, y_hat, average='micro'))\n",
        "\n",
        "# fit all train data\n",
        "decission_tree.fit(X,y)\n",
        "# make prediction using test file to make our prediction '\n",
        "\n",
        "y_hat=decission_tree.predict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rm2_3kebokW",
        "outputId": "d24edb94-83bd-4c58-98f5-c7aec8af08ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 2. 3. 4. 5.] [  2   1  48 369  59]\n"
          ]
        }
      ],
      "source": [
        "# count how many(1,2,3,4,5) in (y_hat)\n",
        "un, co = np.unique(y_hat, return_counts= True)\n",
        "print(un, co )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EhtrnrPLcCz8"
      },
      "outputs": [],
      "source": [
        "# to record the result on file to upload on Kaggle\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(test_data.index), 'rating': y_hat})\n",
        "pred_df.to_csv('pred_1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aMtQJTApiRw"
      },
      "source": [
        "when  I let max depth(defoult ) of the tree , the nodes are expanded until all leaves are pure ,The deeper we allow the tree to grow, the more complex the model will become because we will have more splits and it captures more information about the data and this is one of the reason of overfitting in decision trees because the model will fit perfectly for the training data and will not be able to generalize well on test set. \n",
        "\n",
        "in my result I found that the (f1_score) on training data is about 99.8% but on tasting part of data is low about 67% this mean the model did not learn well .\n",
        "when upload 'pred_1.csv' file my expectation is not very high (f1_score)\n",
        "\n",
        "**here** when I make the predicition using test file and upload the file on Kaggle, The result was that (f1_score)= 0.72384 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drVxl9HRg0tW"
      },
      "source": [
        "#Decision Tree Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKpbnubwg2X0",
        "outputId": "8c1c6df6-3634-464a-f706-92ff7e79a49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction on training data : 0.7825618945102261\n",
            "prediction on test part from training data : 0.7560975609756099\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 5., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4.,\n",
              "       5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 5., 4., 5., 5., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 5., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 5., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 5., 4., 4.,\n",
              "       4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4.,\n",
              "       5., 4., 4., 5., 5., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 4., 4.,\n",
              "       5., 4., 4., 5., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       2., 4., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Decision Tree Classifier with using hyperparametar(max_depth)\n",
        "\n",
        "#import DecisionTreeClassifier from sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# define decission tree model using hyperparamter\n",
        "decission_tree_=DecisionTreeClassifier(max_depth=3)\n",
        "# fit model with train data\n",
        "decission_tree_.fit(X_train, y_train)\n",
        "# make prediction on training data to know how much the model learn from train data \n",
        "y_hat2=decission_tree_.predict(X_train)\n",
        "# predict test part from (train data)\n",
        "y_hat=decission_tree_.predict(X_test)\n",
        "\n",
        "# to check if there is overfitting or no)\n",
        "print(\"prediction on training data :\",f1_score(y_train, y_hat2, average='micro'))\n",
        "print(\"prediction on test part from training data :\",f1_score(y_test, y_hat, average='micro'))\n",
        "# fit all train data\n",
        "decission_tree_.fit(X,y)\n",
        "# make prediction using test file to make our prediction \n",
        "y_hat=decission_tree_.predict(test_data)\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0aPr86yDhQ_w"
      },
      "outputs": [],
      "source": [
        "# to record the result on file to upload on Kaggle\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(test_data.index), 'rating': y_hat})\n",
        "pred_df.to_csv('pred_2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Ii2gbbzWK_"
      },
      "source": [
        "In the previous model when i did not define (max_depth) the model choose the (defoult) whish is the maximum max_depth  that lead to make the model overfitted  ,To avoid this  overfitting, we should reduce the number for max_depth , So I let (max_depth=3) and I found that the model's (f1_score) on training data is not very high this mean that the model did not learn well and (f1_score) on testing part also low\n",
        "and gives me not very high fF1_score)on kaggle.it was 0.78242"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YlOlvUL3GEm"
      },
      "source": [
        "#Decision Tree Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAZcrqBX3co9",
        "outputId": "29021279-f58e-436c-bdef-cdf8b710d0b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction on training data : 0.8385360602798708\n",
            "prediction on test part from training data : 0.75\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 3., 5., 4., 5., 3., 4., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 5., 4., 3., 4., 4., 4., 5., 4., 4., 4., 4.,\n",
              "       5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 3.,\n",
              "       4., 4., 4., 4., 5., 5., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 5., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 5., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 3., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5.,\n",
              "       4., 4., 4., 4., 3., 5., 4., 4., 4., 4., 4., 4., 3., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 3.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 3., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 3., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 5., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 3., 4., 4., 5., 5., 4., 5., 4., 4.,\n",
              "       4., 4., 4., 4., 3., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4.,\n",
              "       5., 4., 4., 5., 3., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 4., 4.,\n",
              "       5., 4., 4., 5., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       5., 4., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#Decision Tree Classifier without using hyperparametar\n",
        "\n",
        "#import DecisionTreeClassifier from sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# define decission tree model\n",
        "decission_tree=DecisionTreeClassifier(max_depth=6,criterion='entropy',splitter='best')\n",
        "\n",
        "#fit model with train data\n",
        "decission_tree.fit(X_train, y_train)\n",
        "\n",
        "# make prediction on training data to know how much the model learn from train data\n",
        "y_hat2=decission_tree.predict(X_train)\n",
        "# predict test part from (train data)\n",
        "y_hat=decission_tree.predict(X_test)\n",
        "\n",
        "# to check if there is overfitting or no)\n",
        "print(\"prediction on training data :\",f1_score(y_train, y_hat2, average='micro'))\n",
        "print(\"prediction on test part from training data :\",f1_score(y_test, y_hat, average='micro'))\n",
        "# fit all train data\n",
        "decission_tree.fit(X,y)\n",
        "# make prediction using test file to make our prediction \n",
        "y_hat=decission_tree.predict(test_data)\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue6NbhN653uu",
        "outputId": "6c118aaf-e23d-4f54-c8ac-6a269c0795b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3. 4. 5.] [ 17 429  33]\n"
          ]
        }
      ],
      "source": [
        "# count how many(1,2,3,4,5) in (y_hat)\n",
        "un, co = np.unique(y_hat, return_counts= True)\n",
        "print(un, co )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LBtF6KUa6IQY"
      },
      "outputs": [],
      "source": [
        "# to record the result on file to upload on Kaggle\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(test_data.index), 'rating': y_hat})\n",
        "pred_df.to_csv('pred_decission_2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtZvrcZr9SqS"
      },
      "source": [
        "**My question here  is how to tune a Decision Tree. What should be the range of values I should try for the maximum depth, which criteria shoulld I use (\"gini\",\"entropy\")\n",
        "which strategy  should used to choose the split at each node.(“best” ,“random” )?**\n",
        "\n",
        "To respond to these questions, I need to know how changing one will influence my model. What does the criterion (gini or entropy) mean when it comes to increasing the maximum depth? I used Information Gain, which uses the entropy measure because it gives the most amount of information, and I assumed that entropy would improve the model's ability to learn from information because it gives the most amount of information, but I found no impact when I replaced the default (gini criteria) with (entropy criteria). I expect that the splitting criteria I choose will not have a significant impact on my model's performance.\n",
        "\n",
        "here we have many of features, so “best” splitter would be ideal because it will calculate the best features to split based on the impurity measure and use that to split the nodes, whereas if choose “random” we have a high chance of ending up with features that don’t really give  much information, which would lead to a more deeper less precise tree. \n",
        "when I used  \"random\" splitter it's effaect made the perfomance less  but \"best\" splitter its effect in performance was good \n",
        "\n",
        "in the previous model when i did not define (max_depth) the model choose the (defoult) whish is the maximum max_depth  that lead to make the model overfitted  ,To avoid this  overfitting, we should reduce the number for max_depth , So I let (max_depth=3) and I found that the model's (f1_score) on training data is not very high this mean that the model did not learn well and (f1_score) on testing part also low ,So I tried to increase (max_depth=6) and it gives me a good performance than (max_depth=3 ) or remain the defoult (max_depth) and it gives me 0.79916 on kaggle\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayse Model"
      ],
      "metadata": {
        "id": "ClJM9plrluxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import GaussianNB from sklearn\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# define decission tree model using hyperparamter\n",
        "params_NB = {'var_smoothing': np.logspace(0,-5, num=100)}\n",
        "NB= GaussianNB()\n",
        "naive_classifier= GridSearchCV(NB,params_NB)\n",
        "# fit model with train data\n",
        "naive_classifier.fit(X_train, y_train)\n",
        "\n",
        "#print the var_smoothing value\n",
        "print(naive_classifier.best_params_)\n",
        "\n",
        "# predict test part from (train data)\n",
        "y_hat=naive_classifier.predict(X_test)\n",
        "\n",
        "# to check if there is overfitting or no)\n",
        "print(\"prediction on test part from training data :\",f1_score(y_test, y_hat, average='micro'))\n",
        "\n",
        "# fit all train data\n",
        "naive_classifier.fit(X,y)\n",
        "# make prediction using test file to make our prediction \n",
        "y_hat=naive_classifier.predict(test_data)\n",
        "y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp5ks08Ml1h-",
        "outputId": "e3b3a86b-cb1f-473b-ec58-0f68574e1be2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'var_smoothing': 1.0}\n",
            "prediction on test part from training data : 0.7073170731707317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to record the result on file to upload on Kaggle\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(test_data.index), 'rating': y_hat})\n",
        "pred_df.to_csv('pred_walkthrough.csv', index=False)"
      ],
      "metadata": {
        "id": "RdzMWnYunhF1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayes algorithm is a straightforward classification method.\n",
        "Tuning the model's hyper-parameters is one of the first things we try to do to increase its performance. The Naive Bayes classifier, on the other hand, has a fairly small parameter set.\n",
        "Because the number of classes is sometimes the only parameter depending on the implementation, hyper-parameter tuning is not a valid strategy for improving Naive Bayes classifier accuracy.\n",
        "So, by applying some simple approaches to the dataset, such as data pretreatment, we may improve the Naive Bayes classifier.\n",
        "\n",
        "**Naïve Bayes is a probabilistic classifier based on Bayes theorem** so we use Laplace smoothing  which is a smoothing technique to handles the problem of zero probability in Naïve Bayes. \n",
        "\n",
        "when I use naive model to make the classification it wasn't the best model \n",
        "because  (f1_score) wasn't high it was 0.66945"
      ],
      "metadata": {
        "id": "101MnKCJoRG2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly4hfgNlB7zW"
      },
      "source": [
        "#SVM Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "skGq55i7DG8D"
      },
      "outputs": [],
      "source": [
        "#make scaler for train data\n",
        "X_train=StandardScaler().fit_transform(X_train)\n",
        "X_test=StandardScaler().fit_transform(X_test)\n",
        "X=StandardScaler().fit_transform(X)\n",
        "# make scaler for test data\n",
        "test_data_sca=StandardScaler().fit_transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bydsyOLrD5vP",
        "outputId": "b3899156-73ee-47d3-95f2-73a9f16a2bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction on training data : 0.9989235737351991\n",
            "prediction on test part from training data : 0.5853658536585366\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5., 4., 3., 5., 4., 3., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 3., 3., 4., 4., 5., 4., 4., 5., 4., 5., 4., 4., 5., 3., 4., 4.,\n",
              "       3., 4., 4., 4., 4., 3., 5., 4., 4., 4., 4., 3., 3., 3., 4., 4., 4.,\n",
              "       3., 3., 3., 4., 3., 4., 3., 3., 4., 4., 4., 4., 4., 5., 4., 4., 3.,\n",
              "       4., 4., 3., 4., 4., 4., 3., 3., 4., 4., 4., 4., 5., 3., 4., 4., 3.,\n",
              "       4., 4., 4., 4., 5., 5., 4., 4., 4., 4., 4., 5., 4., 4., 3., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 5., 5., 3., 4., 4., 4., 4., 5., 3., 4., 5.,\n",
              "       4., 3., 4., 3., 4., 4., 5., 4., 4., 5., 4., 4., 4., 3., 4., 2., 5.,\n",
              "       4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       3., 3., 4., 3., 4., 5., 4., 4., 4., 3., 4., 3., 4., 4., 3., 4., 4.,\n",
              "       4., 4., 4., 3., 2., 4., 4., 4., 4., 4., 5., 4., 3., 4., 3., 4., 4.,\n",
              "       4., 4., 3., 4., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 5., 4., 4., 5., 3., 4., 3., 5., 4., 5., 4., 4., 4., 4., 5., 4.,\n",
              "       4., 5., 4., 4., 4., 5., 5., 5., 4., 4., 4., 4., 4., 4., 5., 4., 3.,\n",
              "       4., 4., 4., 4., 3., 3., 4., 5., 4., 4., 4., 4., 3., 4., 4., 4., 3.,\n",
              "       4., 4., 5., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 3.,\n",
              "       5., 4., 4., 5., 4., 4., 4., 4., 4., 4., 3., 4., 4., 5., 3., 4., 4.,\n",
              "       4., 5., 4., 4., 3., 4., 4., 3., 4., 4., 5., 4., 4., 3., 3., 5., 5.,\n",
              "       4., 4., 4., 3., 4., 4., 4., 4., 4., 3., 4., 3., 4., 4., 3., 4., 3.,\n",
              "       4., 4., 4., 5., 3., 5., 4., 4., 4., 4., 4., 3., 2., 4., 4., 5., 5.,\n",
              "       3., 3., 3., 4., 5., 4., 4., 4., 3., 3., 4., 4., 5., 4., 3., 5., 5.,\n",
              "       4., 4., 4., 4., 4., 3., 4., 4., 4., 3., 4., 4., 4., 3., 4., 4., 4.,\n",
              "       4., 3., 4., 4., 3., 4., 4., 3., 4., 5., 4., 4., 3., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 3., 4., 4., 4., 4., 4., 5., 4., 4., 4., 5., 5., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 5., 4., 3., 5., 4., 4., 5., 3., 4., 4., 4.,\n",
              "       4., 5., 3., 5., 4., 4., 4., 4., 4., 4., 4., 3., 3., 4., 5., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 5., 4., 3., 4., 4., 4., 4., 4., 5., 4., 3., 4.,\n",
              "       3., 3., 4., 5., 4., 4., 4., 4., 4., 4., 4., 5., 4., 5., 4., 4., 4.,\n",
              "       2., 3., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "#imports\n",
        "# import SVC model from sklearn.svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# define SVM model\n",
        "svm=SVC(C=1000,kernel='rbf')\n",
        "\n",
        "# fit model with train data\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# make prediction on training data to know how much the model learn from train data \n",
        "y_hat2=svm.predict(X_train)\n",
        "# predict test part from (train data)\n",
        "y_hat=svm.predict(X_test)\n",
        "\n",
        "# to check if there is overfitting or no)\n",
        "print(\"prediction on training data :\",f1_score(y_train, y_hat2, average='micro'))\n",
        "print(\"prediction on test part from training data :\",f1_score(y_test, y_hat, average='micro'))\n",
        "\n",
        "# make prediction using test file to make our prediction\n",
        "svm.fit(X,y)\n",
        "y_hat=svm.predict(test_data_sca)\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TVOR4M9fFnLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef233c9-6a13-4a59-a505-8c16ab048c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 3. 4. 5.] [  4  86 325  64]\n"
          ]
        }
      ],
      "source": [
        "# count how many(1,2,3,4,5) in (y_hat)\n",
        "un, co = np.unique(y_hat, return_counts= True)\n",
        "print(un, co )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZfJgKFFHF2hn"
      },
      "outputs": [],
      "source": [
        "# to record the result on file to upload on Kaggle\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(test_data.index), 'rating': y_hat})\n",
        "pred_df.to_csv('pred_walkthrough.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C parameterpr** the penalty parameter denotes the phrase misclassification or error. The SVM optimisation uses the misclassification or error term to determine how much mistake is tolerable.\n",
        "When C is high, all data points will be accurately classified; nevertheless, there is a risk of overfitting.\n",
        "**here** in my model when define C with large number the (f1_score) in the prediction of training data part was very high and (f1_score) in the prediction of testing part of data was very low which model may overfitted and (f1_score on kaggle was not high it was 0.65690"
      ],
      "metadata": {
        "id": "-N4koM5j66--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM Model 2"
      ],
      "metadata": {
        "id": "bbRnTbsDL-Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "# import SVC model from sklearn.svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# define  hyperparamter for SVM model\n",
        "param_grid = { 'C':[1,4,5,10,15],'kernel':['rbf']}\n",
        "# define SVM model\n",
        "svm=SVC()\n",
        "classifier2= GridSearchCV(svm, param_grid, refit = True, verbose = 3)\n",
        "# fit model with train data\n",
        "classifier2.fit(X_train, y_train)\n",
        "# print hyperparamter\n",
        "print(\"**************************************\")\n",
        "print(classifier2.best_params_)\n",
        "print(\"**************************************\")\n",
        "\n",
        "# make prediction on training data to know how much the model learn from train data \n",
        "y_hat2=classifier2.predict(X_train)\n",
        "# predict test part from (train data)\n",
        "y_hat=classifier2.predict(X_test)\n",
        "\n",
        "# to check if there is overfitting or no)\n",
        "print(\"prediction on training data :\",f1_score(y_train, y_hat2, average='micro'))\n",
        "print(\"prediction on test part from training data :\",f1_score(y_test, y_hat, average='micro'))\n",
        "\n",
        "# make prediction using test file to make our prediction\n",
        "classifier2.fit(X,y)\n",
        "\n",
        "y_hat=classifier2.predict(test_data_sca)\n",
        "y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en1_MngtMGl_",
        "outputId": "3c99f39d-7ba1-4f79-fb1d-811113ab6fdc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "[CV 1/5] END ...................C=1, kernel=rbf;, score=0.720 total time=   0.2s\n",
            "[CV 2/5] END ...................C=1, kernel=rbf;, score=0.715 total time=   0.1s\n",
            "[CV 3/5] END ...................C=1, kernel=rbf;, score=0.742 total time=   0.1s\n",
            "[CV 4/5] END ...................C=1, kernel=rbf;, score=0.720 total time=   0.1s\n",
            "[CV 5/5] END ...................C=1, kernel=rbf;, score=0.708 total time=   0.1s\n",
            "[CV 1/5] END ...................C=4, kernel=rbf;, score=0.710 total time=   0.1s\n",
            "[CV 2/5] END ...................C=4, kernel=rbf;, score=0.715 total time=   0.1s\n",
            "[CV 3/5] END ...................C=4, kernel=rbf;, score=0.726 total time=   0.1s\n",
            "[CV 4/5] END ...................C=4, kernel=rbf;, score=0.726 total time=   0.1s\n",
            "[CV 5/5] END ...................C=4, kernel=rbf;, score=0.730 total time=   0.1s\n",
            "[CV 1/5] END ...................C=5, kernel=rbf;, score=0.715 total time=   0.1s\n",
            "[CV 2/5] END ...................C=5, kernel=rbf;, score=0.726 total time=   0.2s\n",
            "[CV 3/5] END ...................C=5, kernel=rbf;, score=0.731 total time=   0.1s\n",
            "[CV 4/5] END ...................C=5, kernel=rbf;, score=0.731 total time=   0.1s\n",
            "[CV 5/5] END ...................C=5, kernel=rbf;, score=0.735 total time=   0.1s\n",
            "[CV 1/5] END ..................C=10, kernel=rbf;, score=0.683 total time=   0.1s\n",
            "[CV 2/5] END ..................C=10, kernel=rbf;, score=0.699 total time=   0.2s\n",
            "[CV 3/5] END ..................C=10, kernel=rbf;, score=0.710 total time=   0.2s\n",
            "[CV 4/5] END ..................C=10, kernel=rbf;, score=0.742 total time=   0.3s\n",
            "[CV 5/5] END ..................C=10, kernel=rbf;, score=0.714 total time=   0.3s\n",
            "[CV 1/5] END ..................C=15, kernel=rbf;, score=0.683 total time=   0.2s\n",
            "[CV 2/5] END ..................C=15, kernel=rbf;, score=0.656 total time=   0.2s\n",
            "[CV 3/5] END ..................C=15, kernel=rbf;, score=0.694 total time=   0.2s\n",
            "[CV 4/5] END ..................C=15, kernel=rbf;, score=0.737 total time=   0.2s\n",
            "[CV 5/5] END ..................C=15, kernel=rbf;, score=0.714 total time=   0.2s\n",
            "**************************************\n",
            "{'C': 5, 'kernel': 'rbf'}\n",
            "**************************************\n",
            "prediction on training data : 0.8396124865446717\n",
            "prediction on test part from training data : 0.7195121951219512\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "[CV 1/5] END ...................C=1, kernel=rbf;, score=0.731 total time=   0.2s\n",
            "[CV 2/5] END ...................C=1, kernel=rbf;, score=0.726 total time=   0.2s\n",
            "[CV 3/5] END ...................C=1, kernel=rbf;, score=0.726 total time=   0.3s\n",
            "[CV 4/5] END ...................C=1, kernel=rbf;, score=0.739 total time=   0.2s\n",
            "[CV 5/5] END ...................C=1, kernel=rbf;, score=0.734 total time=   0.1s\n",
            "[CV 1/5] END ...................C=4, kernel=rbf;, score=0.726 total time=   0.1s\n",
            "[CV 2/5] END ...................C=4, kernel=rbf;, score=0.721 total time=   0.3s\n",
            "[CV 3/5] END ...................C=4, kernel=rbf;, score=0.703 total time=   0.3s\n",
            "[CV 4/5] END ...................C=4, kernel=rbf;, score=0.743 total time=   0.2s\n",
            "[CV 5/5] END ...................C=4, kernel=rbf;, score=0.734 total time=   0.2s\n",
            "[CV 1/5] END ...................C=5, kernel=rbf;, score=0.721 total time=   0.2s\n",
            "[CV 2/5] END ...................C=5, kernel=rbf;, score=0.726 total time=   0.1s\n",
            "[CV 3/5] END ...................C=5, kernel=rbf;, score=0.712 total time=   0.2s\n",
            "[CV 4/5] END ...................C=5, kernel=rbf;, score=0.748 total time=   0.1s\n",
            "[CV 5/5] END ...................C=5, kernel=rbf;, score=0.720 total time=   0.2s\n",
            "[CV 1/5] END ..................C=10, kernel=rbf;, score=0.685 total time=   0.2s\n",
            "[CV 2/5] END ..................C=10, kernel=rbf;, score=0.721 total time=   0.2s\n",
            "[CV 3/5] END ..................C=10, kernel=rbf;, score=0.708 total time=   0.2s\n",
            "[CV 4/5] END ..................C=10, kernel=rbf;, score=0.757 total time=   0.2s\n",
            "[CV 5/5] END ..................C=10, kernel=rbf;, score=0.697 total time=   0.2s\n",
            "[CV 1/5] END ..................C=15, kernel=rbf;, score=0.667 total time=   0.2s\n",
            "[CV 2/5] END ..................C=15, kernel=rbf;, score=0.726 total time=   0.3s\n",
            "[CV 3/5] END ..................C=15, kernel=rbf;, score=0.685 total time=   0.2s\n",
            "[CV 4/5] END ..................C=15, kernel=rbf;, score=0.752 total time=   0.2s\n",
            "[CV 5/5] END ..................C=15, kernel=rbf;, score=0.665 total time=   0.2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 5., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 4., 4., 4.,\n",
              "       4., 4., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to record the result on file to upload on Kaggle\n",
        "pred_df = pd.DataFrame(data={'id': np.asarray(test_data.index), 'rating': y_hat})\n",
        "pred_df.to_csv('pred_walkthrough2.csv', index=False)"
      ],
      "metadata": {
        "id": "sMhgl3veMrFu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4AuawLOGVGU"
      },
      "source": [
        "C parameterpr the penalty parameter denotes the phrase misclassification or error. The SVM optimisation uses the misclassification or error term to determine how much mistake is tolerable. \n",
        " here in my model when define C with small number (C=5) the (f1_score) in the prediction of training data part was about 0.83 and (f1_score) in the prediction of testing part of data was about 0.71 \n",
        "and gives me 0.75313 on Kaggle \n",
        "I used GridSearch to get the best hyperparameter (the best C) to get the higher performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqNq5ZPaiFVt"
      },
      "source": [
        "#Problem Formulation:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOEHM7ini-vp"
      },
      "source": [
        "**Define The problem**\n",
        "\n",
        "The problem is that We have  a data from wish.com\n",
        "the data contain some available data ,nosies and mising value which need to make preprocessing on it \n",
        "and we want to pridect the ratings which  are in categories from 1 to 5.\n",
        "\n",
        "**inputs**\n",
        "\n",
        "our inputs are some features from wish.com which use to help us to get Rating Prediction \n",
        "\n",
        "**outputs**\n",
        "\n",
        "our output is the product's rating from 1 to 5 \n",
        "\n",
        "**What data mining function is requride?**\n",
        "\n",
        " we use classification function (decision tree classifier, naive bayes classifier , SVM classifier) models to get Rating Prediction of our product\n",
        "\n",
        " **What could be the challenges?**\n",
        "\n",
        "\n",
        "\n",
        "1.   The challenge is that we need to make preprocessing in our data\n",
        "2.   fill nun values, remove outliers,drop columns that are useless and have not an impact in data \n",
        "\n",
        "1.   choose the best hyper paramter in model to get high (f1_score) accuracy\n",
        "\n",
        "**What is the impact?**\n",
        "\n",
        "When Wish.com be able to classify the products rating itsel befor peaple doing  by using models .in this case Wish.com can offer products that have high rating which customers need  and this will increase sells \n",
        "\n",
        "**What is an ideal solution?**\n",
        "\n",
        "The ideal solution is that, make the model learn well and could classify the product in a right way via making a good preprocessing for the given data and insertion these data to classification models to make a right classification for the product\n",
        "I tried to make classification using (Decission tree , SVm and Naive bayse) and find the best of them is Decission tree because it gives me the higher (f1_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Questions nswers"
      ],
      "metadata": {
        "id": "HVNWBj9w0ie9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Data Mining is a misnomer? What is another preferred name?**\n",
        "\n",
        "Datab Data Mining is a misnomer because we did not mining for the data because we get the data from collection process \n",
        "but we mining for the data so the  proper name is knowledge descovry from the data.\n",
        "\n",
        "another preferred name for data mining is Knowledge discovery (mining) in databases (KDD), \n",
        "knowledge extraction, data analysis, data archeology, data dredging, information harvesting,business intelligence\n",
        "\n",
        "**What is the general knowledge discovery process? What is the difference between a data engineer and data scientist/AI engineer?**\n",
        "\n",
        "the general knowledge discovery proces is \n",
        "\n",
        "**1-Understanding application domain**\n",
        "\n",
        "**2-Selection:**choose a data set, or focus on a subset of variables\n",
        "\n",
        "**3-preprocessing and cleaning:**\n",
        "    Noise and outliers are removed.\n",
        "    Getting the data you need to model or account for noise.\n",
        "    Methods for dealing with data fields that are missing.\n",
        "\n",
        "**4-Transformation:**\n",
        "    Depending on the task's purpose, choose suitable characteristics to represent the data.\n",
        "    reduce the effective number of variables under consideration or to find invariant representations for the data by Using dimensionality reduction or transformation methods\n",
        "\n",
        "**5-Data Mining:**\n",
        "which are an algorithms used to extract significant patterns from altered data, which aid in the development of prediction models.\n",
        "\n",
        "**6-Evaluation:**\n",
        "It is the process of recognising the increasing patterns that indicate knowledge based on predetermined metrics.\n",
        "\n",
        "**7-Knowledge representation**:\n",
        "\n",
        "This is the final phase in the KDD process, and it demands that the 'knowledge' retrieved in the previous step be applied to a specific application\n",
        "or domain in a visual format such as tables,reports, and so on\n",
        "\n",
        "he difference between a data engineer and data scientist/AI engineer is\n",
        "\n",
        "**Data engineers:** are data specialists who provide big data infrastructure for data scientists to make analysis\n",
        "\n",
        "**Data scientist :** The job of a data scientist is to conversion raw data into cleverness  and knowledge.\n",
        "They dealing with scientific challenges using statistics, machine learning, and analytic methodologies.\n",
        "\n",
        "**in data mining, what is the difference between prediction and categorization?**\n",
        "\n",
        "categorization: is The process of determining the category or class label to which a new observation belongs to\n",
        "\n",
        "prediction:is the process of recognising the numerical data that is absent or unavailable for a new observation.\n",
        "Prediction is unaffected by the class label,\n",
        "\n",
        "**Why data science/machine learning is a bad idea in the context of information security?** \n",
        "\n",
        "because it increase uncertainty and increase risk of data breach and fine\n",
        "Difficult to evaluate change in AI\n",
        "Difficult to verify against compliance\n",
        "\n",
        "**What is CIA principle and how can we use it to access the security/privacy aspect of the AI system/pipelines?**\n",
        "\n",
        "CIA  is stand for confidentiality, integrity, and availability \n",
        "these three principles represent very importance for  any organization’s security infrastructure\n",
        "so they should be goals and objectives for every security program.\n",
        "\n",
        "confidentiality: suitable procedures to secure sensitive data and maintain management\n",
        "\n",
        "integrity: method is used to stop losing data or being altered \n",
        "\n",
        "availability: information can be reached easily when individuals need it \n",
        "Applying the  CIA Principles is depending on an organization’s security goals and the nature of the business,and any relevant regulatory constraints.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tP6FzI1H1F-j"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4QZ4mtcZWQRM",
        "Axljt_8iWbss",
        "toAt_BrRWrHP",
        "VomX0IvmY3Ss",
        "QCbMn5sdZcHX",
        "t0zM2jEYbIjC",
        "39lh2CLaatU5",
        "drVxl9HRg0tW",
        "5YlOlvUL3GEm",
        "ly4hfgNlB7zW",
        "bbRnTbsDL-Jy",
        "mqNq5ZPaiFVt"
      ],
      "name": "Competition #1 .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}